{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0482d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Model Evaluation Summary ===\n",
      "                 Model  Precision  Recall  F1-Score  ROC-AUC\n",
      "0  Logistic Regression      0.928   0.918     0.923    0.979\n",
      "1        Decision Tree      0.827   0.878     0.851    0.921\n",
      "2        Random Forest      0.967   0.898     0.931    0.981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# ---------------------------\n",
    "# 1. DATA PREPROCESSING & BASELINE MODEL TRAINING\n",
    "# ---------------------------\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/yasserjaghoori/Desktop/Grad School Classes/Spring 2025/AIT 736/Final Project/Credit Card Dataset.csv')\n",
    "\n",
    "# Separate the features (X) from the target variable (y)\n",
    "# 'Class' is our target where 1 = fraud, 0 = non-fraud\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Standardize 'Time' and 'Amount' columns\n",
    "# All other columns are already PCA-transformed and scaled\n",
    "# We scale these two so their magnitude doesn't bias the model\n",
    "scaler = StandardScaler()\n",
    "X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])\n",
    "\n",
    "# Handle class imbalance by downsampling the majority class (non-fraud)\n",
    "# If we don't do this, the model will mostly learn to predict \"not fraud\"\n",
    "df_majority = df[df.Class == 0]  # Majority class (non-fraud)\n",
    "df_minority = df[df.Class == 1]  # Minority class (fraud)\n",
    "\n",
    "# Randomly downsample the majority class to a 5:1 ratio\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority) * 5,  # Keep 5x more non-fraud than fraud\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the downsampled majority class with the full minority class\n",
    "# This results in a smaller but more balanced dataset for training\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Recreate the X and y from the balanced dataset\n",
    "X_balanced = df_balanced.drop('Class', axis=1)\n",
    "y_balanced = df_balanced['Class']\n",
    "\n",
    "# Scale 'Time' and 'Amount' again because X_balanced is a new DataFrame\n",
    "X_balanced[['Time', 'Amount']] = scaler.fit_transform(X_balanced[['Time', 'Amount']])\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "# Stratify ensures the fraud-to-nonfraud ratio is maintained in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Define the models to train\n",
    "# Logistic Regression = interpretable baseline\n",
    "# Decision Tree = captures non-linear patterns\n",
    "# Random Forest = ensemble method for better generalization\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store model performance results for the baseline models\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each baseline model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Predict class labels\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Get predicted probabilities\n",
    "\n",
    "    # Generate classification metrics and ROC-AUC score\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    report['roc_auc'] = auc_score\n",
    "\n",
    "    results[name] = report  # Store results\n",
    "\n",
    "# Extract fraud detection metrics (for Class 1) from the baseline models\n",
    "summary_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    fraud_metrics = metrics['1']  # Class 1 = Fraud\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Precision': round(fraud_metrics['precision'], 3),  # Out of predicted frauds, how many were correct\n",
    "        'Recall': round(fraud_metrics['recall'], 3),        # Out of actual frauds, how many we caught\n",
    "        'F1-Score': round(fraud_metrics['f1-score'], 3),      # Balance of precision and recall\n",
    "        'ROC-AUC': round(metrics['roc_auc'], 3)             # Overall probability ranking quality\n",
    "    })\n",
    "\n",
    "# Create and display summary DataFrame for baseline models\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"=== Baseline Model Evaluation Summary ===\")\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42df064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "=== Best Ensemble Voting Classifier Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       493\n",
      "           1       0.89      0.92      0.90        98\n",
      "\n",
      "    accuracy                           0.97       591\n",
      "   macro avg       0.94      0.95      0.94       591\n",
      "weighted avg       0.97      0.97      0.97       591\n",
      "\n",
      "ROC-AUC Score: 0.983\n",
      "\n",
      "=== Extended Evaluation Metrics for All Models ===\n",
      "                 Model  Precision  Recall  F1-Score  AUPRC  AUC-ROC\n",
      "0  Logistic Regression      0.928   0.918     0.923  0.951    0.979\n",
      "1        Decision Tree      0.827   0.878     0.851  0.862    0.921\n",
      "2        Random Forest      0.967   0.898     0.931  0.951    0.981\n",
      "3    Ensemble (Voting)      0.891   0.918     0.905  0.949    0.983\n",
      "\n",
      "--- Confusion Matrices ---\n",
      "\n",
      "Logistic Regression:\n",
      "[[486   7]\n",
      " [  8  90]]\n",
      "\n",
      "Decision Tree:\n",
      "[[475  18]\n",
      " [ 12  86]]\n",
      "\n",
      "Random Forest:\n",
      "[[490   3]\n",
      " [ 10  88]]\n",
      "\n",
      "Ensemble (Voting):\n",
      "[[482  11]\n",
      " [  8  90]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== ENSEMBLE MODEL USING VOTING CLASSIFIER + SMOTE + GRIDSEARCH ==========\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define base classifiers with class weighting to further handle imbalance\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "tree = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "forest = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Create a soft voting classifier (uses average of predicted probabilities)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('dt', tree), ('rf', forest)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Build a pipeline with SMOTE and the VotingClassifier to address imbalance in each fold\n",
    "ensemble_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', voting_clf)\n",
    "])\n",
    "\n",
    "# Use Stratified K-Fold cross-validation to maintain class balance during tuning\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for tuning the ensemble\n",
    "param_grid = {\n",
    "    'classifier__lr__C': [0.1, 1.0],\n",
    "    'classifier__dt__max_depth': [None, 10],\n",
    "    'classifier__rf__n_estimators': [100, 200],\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV to find the best ensemble model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ensemble_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set using the best ensemble model\n",
    "best_ensemble = grid_search.best_estimator_\n",
    "y_pred_ensemble = best_ensemble.predict(X_test)\n",
    "y_proba_ensemble = best_ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Generate final classification report and ROC-AUC for the ensemble model\n",
    "ensemble_report = classification_report(y_test, y_pred_ensemble)\n",
    "ensemble_auc = roc_auc_score(y_test, y_proba_ensemble)\n",
    "\n",
    "print(\"\\n=== Best Ensemble Voting Classifier Report ===\")\n",
    "print(ensemble_report)\n",
    "print(\"ROC-AUC Score:\", round(ensemble_auc, 3))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. EXTENDED EVALUATION: CONFUSION MATRIX, AUPRC, AND AUC-ROC\n",
    "# ---------------------------\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def extended_evaluation(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on test data, computing:\n",
    "      - Confusion Matrix\n",
    "      - Precision, Recall, and F1-score for the fraud class (Class 1)\n",
    "      - Area Under the Precision-Recall Curve (AUPRC)\n",
    "      - Area Under the ROC Curve (AUC-ROC)\n",
    "    \"\"\"\n",
    "    # Predict class labels and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report to extract precision, recall, and F1-score for Class 1\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision_val = report['1']['precision']\n",
    "    recall_val = report['1']['recall']\n",
    "    f1_val = report['1']['f1-score']\n",
    "    \n",
    "    # Compute ROC curve and AUC-ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc_roc_val = auc(fpr, tpr)\n",
    "    \n",
    "    # Compute Precision-Recall curve and AUPRC\n",
    "    precisions, recalls, _ = precision_recall_curve(y_test, y_proba)\n",
    "    auprc_val = auc(recalls, precisions)\n",
    "    \n",
    "    return cm, precision_val, recall_val, f1_val, auprc_val, auc_roc_val\n",
    "\n",
    "# Evaluate each baseline model using the extended metrics\n",
    "evaluation_results = {}\n",
    "for model_name, model in models.items():\n",
    "    cm, prec, rec, f1, auprc, auc_roc_val = extended_evaluation(model, X_test, y_test)\n",
    "    evaluation_results[model_name] = {\n",
    "        'Confusion Matrix': cm,\n",
    "        'Precision': round(prec, 3),\n",
    "        'Recall': round(rec, 3),\n",
    "        'F1-Score': round(f1, 3),\n",
    "        'AUPRC': round(auprc, 3),\n",
    "        'AUC-ROC': round(auc_roc_val, 3)\n",
    "    }\n",
    "\n",
    "# Evaluate the ensemble model using the extended metrics\n",
    "cm, prec, rec, f1, auprc, auc_roc_val = extended_evaluation(best_ensemble, X_test, y_test)\n",
    "evaluation_results['Ensemble (Voting)'] = {\n",
    "    'Confusion Matrix': cm,\n",
    "    'Precision': round(prec, 3),\n",
    "    'Recall': round(rec, 3),\n",
    "    'F1-Score': round(f1, 3),\n",
    "    'AUPRC': round(auprc, 3),\n",
    "    'AUC-ROC': round(auc_roc_val, 3)\n",
    "}\n",
    "\n",
    "# Compile all extended evaluation results into a summary DataFrame for comparison\n",
    "summary_data_extended = []\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    summary_data_extended.append({\n",
    "        'Model': model_name,\n",
    "        'Precision': metrics['Precision'],\n",
    "        'Recall': metrics['Recall'],\n",
    "        'F1-Score': metrics['F1-Score'],\n",
    "        'AUPRC': metrics['AUPRC'],\n",
    "        'AUC-ROC': metrics['AUC-ROC']\n",
    "    })\n",
    "\n",
    "summary_df_extended = pd.DataFrame(summary_data_extended)\n",
    "print(\"\\n=== Extended Evaluation Metrics for All Models ===\")\n",
    "print(summary_df_extended)\n",
    "\n",
    "# Optionally, print the confusion matrices for each model\n",
    "print(\"\\n--- Confusion Matrices ---\")\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(metrics['Confusion Matrix'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
